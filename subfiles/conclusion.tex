% !TEX root =../dissertation.tex
\documentclass[./dissertation.tex]{subfiles}
\begin{document}
\chapter{Conclusion}
\label{ch:conclusion}
% This is where your Conclusion will go

\section{Summary of Contributions}
This dissertation systematically addresses key challenges in biomedical image segmentation by developing and evaluating supervised, self-supervised, minimally supervised, and foundation model-based segmentation methods. Collectively, these chapters form a cohesive framework that advances segmentation accuracy, generalizability, and usability across diverse biomedical imaging scenarios.

In Chapter \ref{ch:toxo}, we introduced TSeg, a comprehensive pipeline for 3D cell segmentation, tracking, and motility analysis, with a particular focus on \textit{Toxoplasma gondii}. TSeg combines deep learning-based segmentation with a user-friendly interface, enabling researchers with limited coding expertise to readily deploy CNN models. Its robust performance across various cell types and imaging modalities highlights its potential as a broadly applicable tool in biomedical research.

Chapter \ref{ch:cilia} presented a self-supervised segmentation method that generates pseudo-labels via optical flow and autoregressive modeling. This approach reduces the reliance on manually annotated datasets while maintaining strong segmentation performance. Although demonstrated specifically for cilia segmentation, the method can be extended to other time-series biomedical imaging tasks, such as cardiac motion analysis.

In Chapter \ref{ch:contrastive}, we leveraged contrastive learning to enhance minimally supervised segmentation. This self-supervision approach demonstrated the potential for learning discriminative representations using only minimal labeled data. One future direction is to investigate whether our contrastive method could serve as an alternative to standard contrastive loss functions (e.g., DHN loss) in models like BiomedCLIP, potentially improving their performance in biomedical applications.

Chapter \ref{ch:foundation} explored the role of foundation models, focusing on the adaptation of Segment Anything Models (SAMs) for biomedical images. While SAMs provide flexible and efficient segmentation solutions, our findings emphasize the need for domain-specific fine-tuning. Nonetheless, SAMs represent an accessible tool for generating sufficiently accurate segmentations with minimal model training, opening avenues for broader adoption in biomedical image analysis.

\section{Theoretical and Practical Implications}
The proposed methodologies contribute both theoretically and practically to biomedical image segmentation. TSeg serves as a complete segmentation and tracking solution applicable to various 3D cellular imaging tasks. Its GUI-driven design lowers the barrier to entry for non-experts, accelerating the adoption of deep learning techniques in biomedical research.

The self-supervised motion-based segmentation method introduces an efficient way of generating training datasets by leveraging optical flow and autoregressive modeling. This reduces annotation workload without sacrificing segmentation quality, making it well suited for time-series imaging tasks in fields such as cardiology and developmental biology.

Our contrastive learning framework illustrates the efficacy of self-supervision in biomedical segmentation. Future work may explore its integration into existing large-scale biomedical models, potentially boosting their ability to capture subtle morphological differences in complex imaging data.

Finally, the investigation of foundation models underlines both their promise and their limitations in specialized imaging domains. While generic vision models often overlook domain-specific details, appropriately fine-tuned foundation models can yield competitive and scalable solutions. Their ability to deliver sufficiently accurate segmentation with minimal supervision underscores their potential in widespread biomedical applications.

\section{Limitations and Future Directions}
Despite the improvements introduced, several challenges and limitations remain. First, each methodâ€™s performance partly depends on the availability and diversity of training data; domain shifts or highly heterogeneous datasets can degrade accuracy. Second, while self-supervision and foundation models reduce annotation burdens, some level of expert validation is often still necessary to ensure biological or clinical relevance. Finally, the scalability of these methods in large-scale or real-time imaging contexts requires additional exploration.

Future research could focus on extending these frameworks to handle multi-modal data (e.g., combining imaging with molecular or clinical data), exploring more advanced architectures that incorporate temporal or spatial priors, and refining domain adaptation strategies for foundation models. Incorporating uncertainty estimation and explainability methods may also enhance clinical acceptance and interpretability.

\section{Broader Impact and Scientific Contributions}
This dissertation advances biomedical image segmentation by introducing tools and methods that improve accessibility, reduce annotation demands, and enhance model robustness. The potential impact spans multiple domains:

\begin{itemize}
    \item \textbf{Clinical Diagnostics:} Automated segmentation can facilitate the detection and analysis of conditions such as ciliopathies, cancer, and neurodegenerative disorders, streamlining clinical workflows and improving patient care.
    \item \textbf{Biomedical Research:} The proposed segmentation pipelines can be applied to study cellular morphology, motility patterns, and disease progression, accelerating research in areas like developmental biology, immunology, and infectious disease.
    \item \textbf{AI in Healthcare:} By making segmentation tools more accessible and less reliant on large annotated datasets, this work supports broader adoption of AI-driven diagnostics and personalized medicine, potentially transforming how we approach healthcare challenges.
\end{itemize}

In conclusion, these contributions establish a solid foundation for more scalable, efficient, and interpretable biomedical segmentation approaches. By integrating deep learning, self-supervision, and foundation models, this dissertation brings the field closer to practical, domain-ready, and data-efficient solutions that can serve a wide range of medical and biological research endeavors.








% This dissertation systematically addressed critical challenges in biomedical image segmentation by developing and evaluating supervised, self-supervised, minimally supervised, and foundational segmentation methodologies. The primary goal was to enhance segmentation accuracy, generalizability, and practical usability across diverse biomedical imaging scenarios.

% In Chapter \ref{ch:toxo}, we introduced TSeg, a comprehensive pipeline explicitly designed for 3D cell segmentation, tracking, and motility analysis, exemplified by \textit{Toxoplasma gondii}. TSeg integrates robust CNN-based segmentation models with intuitive GUI-based workflows, enabling high accuracy and broad applicability, even for non-expert users. Evaluation on diverse Cell Tracking Challenge datasets demonstrated that TSeg provides robust performance across multiple cell types and imaging modalities, highlighting its potential for broader adoption in biomedical research and clinical applications.

% Chapter \ref{ch:contrastive} explored a novel self-supervised segmentation approach utilizing motion-derived pseudo-labels to overcome the need for extensive manual annotations in cilia segmentation. This method leveraged optical flow and autoregressive modeling, effectively capturing motion patterns to generate reliable segmentation masks. The approach was validated using dyskinetic cilia datasets, demonstrating that self-supervised methods can significantly reduce annotation burdens without compromising accuracy. This technique represents a promising direction for cost-effective and efficient biomedical segmentation.

% In Chapter \ref{ch:foundation}, we advanced minimally supervised segmentation through contrastive learning techniques. By employing contrastive learning frameworks, we substantially reduced reliance on annotated data, demonstrating robust performance across various biomedical datasets. This approach notably improved generalization, facilitating accurate segmentation with significantly fewer annotations. The experimental results underscored the potential of minimally supervised learning methods to address data scarcity challenges inherent in biomedical imaging tasks.

% Finally, Chapter 5 investigated the adaptation of foundational segmentation models, specifically the Segment Anything Model (SAM), to biomedical image segmentation tasks. Through careful fine-tuning strategies, including dataset-specific masking and captioning approaches, we demonstrated substantial improvements in SAM's performance, enabling its application across diverse biomedical imaging modalities. The outcomes indicated that foundational models could effectively bridge the gap between general-purpose and domain-specific segmentation tasks, significantly enhancing segmentation flexibility and accessibility.

% Overall, the findings of this dissertation contribute significantly to biomedical image segmentation by demonstrating practical solutions that enhance accuracy, reduce annotation requirements, improve model generalization, and ensure usability in real-world biomedical applications. Future research directions include further refining foundational model fine-tuning strategies, exploring more sophisticated self-supervised approaches, and extending minimally supervised methods to additional biomedical imaging challenges. Such advancements promise to accelerate biomedical research and clinical practice, making accurate and efficient segmentation broadly accessible and impactful.
\end{document}
