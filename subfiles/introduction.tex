% !TEX root =../dissertation.tex
\documentclass[./dissertation.tex]{subfiles}
\begin{document}




\chapter{Introduction}
\label{ch:intro}

\section{In Search of a Universal Biomedical Segmentation Model}
% \textcolor{BulldogRed}{
Biomedical image segmentation is critical for accurate diagnosis, treatment planning, and biological research, as it precisely delineates anatomical structures and pathological changes. Accurate segmentation enables quantitative analyses, enhancing objectivity, consistency, and reproducibility in medical assessments. Historically, image segmentation relied on traditional methods such as thresholding, watershed segmentation, and optical flow, primarily utilizing pixel intensity to differentiate image regions. However, these traditional techniques frequently encountered significant difficulties when applied to noisy imaging conditions, complex anatomical structures, or subtle intensity gradients, severely limiting their practical applicability in clinical and research settings. Addressing these limitations necessitated sophisticated computational approaches, driving substantial methodological evolution toward advanced image segmentation strategies \cite{isensee2021nnu, hatamizadeh2022unetr}.

% } \textcolor{Olympic}{Get rid of this sentence please!} Image segmentation started with traditional methods and algorithms such as thresholding, watershed, and optical flow, which rely on pixel intensity.

% \textcolor{BulldogRed}{
Traditional rule-based segmentation methods, including thresholding and watershed algorithms, operate by direct pixel intensity analysis to define boundaries but exhibit limited robustness in challenging imaging conditions characterized by noise or poor contrast. Machine learning-based segmentation techniques, such as Support Vector Machines (SVMs) \cite{cortes1995support}, Random Forest classifiers \cite{breiman2001random}, and contrastive learning methods \cite{chen2020simple}, subsequently enhanced segmentation capabilities through statistical pattern recognition. These methods provided improved flexibility and accuracy by learning complex data patterns from annotated examples. Despite these advancements, traditional machine learning methods required intensive manual feature engineering, constraining scalability and limiting generalization across diverse biomedical datasets and modalities.
% } \textcolor{Olympic}{Get rid of this two sentences please!} Traditional rule-based methods like Thresholding and Watershed analyze pixel values to identify borders and boundaries within areas of interest. Machine learning-based methods - such as Support Vector Machines (SVM), random forests, and contrastive learning - involve the use of statistical machine learning models and increased the popularity and applicability of segmentation.

% \textcolor{BulldogRed}{
The advent of deep learning approaches effectively addressed these limitations, signifying a transformative shift in image segmentation research. Deep learning-based segmentation methods, particularly those leveraging Convolutional Neural Networks (CNNs), revolutionized the field by automatically learning hierarchical feature representations directly from raw images, thus eliminating the need for manual feature extraction. CNNs inherently learn both low-level spatial features and high-level semantic features, significantly enhancing segmentation accuracy, consistency, and robustness across diverse biomedical datasets. These deep learning-based segmentation techniques typically fall into supervised, semi-supervised, and unsupervised categories, differentiated by the amount and type of annotated data required. Among supervised methods, the U-Net architecture \cite{ronneberger2015u}, specifically developed for biomedical image segmentation, has emerged as a cornerstone due to its highly effective feature extraction and superior generalization capabilities \cite{chen2021transunet}, setting foundational benchmarks for current segmentation research.
% } \textcolor{Olympic}{Get rid of these four sentences please!} Finally, deep learning-based methods leverage neural networks to learn hierarchical feature representation from raw images without requiring manual feature engineering. This process saw significant improvement with the introduction of Convolutional Neural Networks (CNN). CNNs are trained to detect features in regions of interest, enabling them to perform similarly on new images. Segmentation techniques can be divided into three categories: supervised, semi-supervised, and unsupervised.

% \textcolor{BulldogRed}{
U-Net and its variants have achieved widespread adoption in biomedical image segmentation due to their exceptional ability to automate feature extraction directly from images without manual intervention or extensive preprocessing. Variants include foundational architectures such as the original U-Net \cite{ronneberger2015u} and 3D U-Net \cite{cciccek20163d}, specialized for volumetric data segmentation in computed tomography (CT) and magnetic resonance imaging (MRI). Advanced variants introduce architectural enhancements \cite{cao2022swin}, including Attention U-Net for selective feature refinement \cite{oktay2018attention}, Inception U-Net for multi-scale context capture \cite{zhang2020munet}, Residual U-Net for deeper training capabilities \cite{alom2018recurrent}, and Dense U-Net promoting extensive feature reuse to enhance accuracy and training efficiency \cite{guan2019fully}. Collectively, these CNN-based architectures currently represent the state-of-the-art in biomedical image segmentation, significantly advancing diagnostic precision and enabling more reliable quantitative analyses in medical and biological research.

% } \textcolor{Olympic}{Get rid of these seven sentences please!} U-Net and its variants are extensively adopted in biomedical image segmentation for their capability to automatically extract features from images without manual intervention or preprocessing. They can learn high-level semantic information and low-level spatial information from large-scale data. U-Net architectures are categorized based on their design and functionality. Basic U-Nets, like the original U-Net and 3D U-Net, are foundational, with the latter extending to 3D data for volumetric segmentation useful in CT and MRI scans. Advanced U-Nets include Attention U-Net, which uses attention mechanisms for precision; Inception U-Net, capturing multi-scale information through varied kernel sizes; Residual U-Net, which incorporates residual connections to aid deep network training; and Dense U-Net, promoting feature reuse via dense connections. Currently, CNNs represent the state-of-the-art in image segmentation, with U-Net being the predominant architecture, especially in the field of biomedical image segmentation. These advancements have greatly improved the accuracy and efficiency of biomedical image analysis, making it an essential tool in various fields of research.

% \textcolor{BulldogRed}{
Biomedical images span diverse imaging modalities, each presenting unique segmentation challenges due to varying image characteristics, resolution, noise levels, and structural complexity. Clinical imaging modalities, including CT, MRI, positron emission tomography (PET), and ultrasound imaging, each feature distinct properties affecting segmentation, such as variable tissue contrast, noise artifacts, and differing spatial resolutions \cite{huang2022multi}. Microscopy imaging modalities, such as fluorescence microscopy, bright-field microscopy, phase-contrast microscopy, and electron microscopy, present unique challenges, including intricate subcellular structures, variable staining methods, and diverse cellular morphologies. Given the extensive biological diversity, segmentation targets range widely from microscopic features like nuclei, mitochondria, and cilia to macroscopic anatomical structures like tumors, lesions, blood vessels, bones, brain anatomy. This combination of biological variability and modality-specific imaging challenges underscores the necessity for specialized yet adaptable deep learning models \cite{isensee2021nnu} capable of efficiently generalizing across multiple biomedical segmentation tasks without extensive reconfiguration.
% } \textcolor{Olympic}{Get rid of these four sentences please!} Biomedical images come in a vast variety of formats, types, and modalities. The modalities in medical imaging include computed tomography (CT), magnetic resonance imaging (MRI), positron emission tomography (PET), and ultrasound, while microscopy modalities include fluorescent microscopy, bright-field, lens-free microscopy, light microscopy, volume electron microscopy, and phase contrast microscopy, just to name a few. Similarly, due to the variety of biological structures, segmentation targets can vary from nuclei and cell membranes to organelles such as mitochondria, cilia, tumors, and lesions, as well as blood vessels, bone, and brain structures. This diversity in imaging techniques and segmentation targets highlights the need for specialized and customizable deep learning models in biomedical applications.

% \textcolor{BulldogRed}{
Recently, universal segmentation frameworks, exemplified by the Segment Anything Model (SAM) \cite{kirillov2023segment}, have emerged as significant advancements aiming to generalize segmentation tasks across diverse imaging domains. SAM leverages flexible user inputs, including single points, bounding boxes, and textual descriptions, enabling robust segmentation performance without requiring specialized retraining or extensive annotated datasets. Its effectiveness arises from a transformer-based architecture combined with extensive pre-training on large-scale, diverse image datasets, facilitating segmentation even of previously unseen biomedical structures and modalities, thus significantly reducing the practical barriers for broad biomedical adoption \cite{ma2024segment}.
% } \textcolor{Olympic}{Get rid of these three sentences please!} The Segment Anything Model (SAM) can segment an object within an image using user inputs, including a single point, multiple points, an entire mask, a bounding box, or textual descriptions. This functionality is based on the model's inherent ability to recognize objects, which enables it to segment unfamiliar object types without further training, effectively supporting zero-shot learning. Furthermore, the effectiveness of SAM is enhanced by its specialized architecture and the use of a significantly large dataset.

% \textcolor{BulldogRed}{
The central objective of this dissertation is to systematically address existing gaps in biomedical image segmentation methodologies, explicitly focusing on enhancing model generalizability, reducing dependency on annotated datasets, and improving usability for practical biomedical applications. Specifically, the research investigates three interconnected questions:
\begin{enumerate}
    \item How can supervised segmentation methods be optimized to enhance performance and generalization across diverse biomedical datasets?
    \item In what ways can unsupervised and self-supervised learning techniques effectively reduce the need for extensive annotated biomedical datasets without compromising segmentation accuracy?
    \item How can universal segmentation frameworks be adapted and validated for robust performance in diverse biomedical imaging scenarios?
\end{enumerate}

% }

% \textcolor{BulldogRed}{

The dissertation is organized to comprehensively address these research questions through a structured approach, encompassing supervised, self-supervised, and foundation-model-driven segmentation techniques. The chapters are organized as follows:

\begin{itemize}

    \item \textbf{Chapter \ref{ch:toxo}} introduces TSeg, a novel supervised segmentation pipeline explicitly designed for 3D cell instance segmentation, tracking, and motility classification, particularly optimized for noisy microscopy data such as those of \textit{Toxoplasma Gondii}. This chapter directly addresses challenges of robust supervised segmentation and practical usability, showcasing TSeg's effectiveness and generalizability to diverse cell types through user-friendly interfaces and optimized workflows.

    \item \textbf{Chapter \ref{ch:cilia}} explores self-supervised segmentation methods, proposing an innovative pseudo-labeling strategy to significantly reduce annotation dependency. Utilizing motion-derived masks to generate pseudo-labels, this method addresses the scarcity of annotated datasets, particularly demonstrated in challenging cilia segmentation tasks where annotation efforts are costly and complex.

    \item \textbf{Chapter \ref{ch:contrastive}} investigates minimally supervised segmentation approaches leveraging contrastive learning frameworks. By training on minimal labeled data, this chapter critically assesses and demonstrates the capability of contrastive learning to improve generalization and robustness across diverse biomedical imaging modalities, explicitly addressing the challenges of data scarcity and limited generalizability.

    \item \textbf{Chapter \ref{ch:foundation}} focuses on adapting and validating universal segmentation frameworks, specifically examining the Segment Anything Model (SAM) and its applicability to biomedical contexts. Through prompt engineering and strategic fine-tuning, this chapter systematically addresses domain-specific adaptation challenges and evaluates the robustness, effectiveness, and reproducibility of SAM-based segmentation across varied biomedical imaging tasks.

    \item Finally, \textbf{Chapter \ref{ch:conclusion}} synthesizes the methodological advancements described in previous chapters, critically discusses their broader implications for biomedical imaging research and clinical applications, identifies persistent gaps, and proposes promising directions for future research in biomedical image segmentation.

\end{itemize}

% Chapter 2 rigorously evaluates supervised deep learning segmentation methods, emphasizing performance optimization, generalization improvement, and methodological enhancements across diverse biomedical imaging datasets. Chapter 3 explores unsupervised and self-supervised segmentation approaches, explicitly focusing on methods that reduce reliance on annotated data, critically evaluating their effectiveness and generalizability. Chapter 4 focuses on adapting and validating universal segmentation frameworks such as SAM, investigating their applicability, robustness, and performance across various biomedical modalities and segmentation targets. Finally, Chapter 5 synthesizes these findings, discusses implications, and suggests directions for future biomedical segmentation research.

% }

% \textcolor{BulldogRed}{
Despite these methodological advancements, significant barriers persist, hindering wide-scale adoption of deep learning segmentation methods in biomedical imaging. These ongoing challenges include limited model generalizability, heavy reliance on extensive annotated datasets \cite{wang2021annotation}, poor transferability across distinct biomedical imaging modalities, and practical usability constraints, especially for users lacking extensive computational expertise. The following "Challenges" section delves deeper into these issues, setting the stage for subsequent chapters, which systematically propose, develop, and evaluate novel solutions addressing these critical barriers.
% }

\section{Challenges}
% \textcolor{BulldogRed}{
Biomedical image segmentation, particularly with deep learning models, has seen significant advancements; however, numerous practical and theoretical challenges remain, hindering wider clinical and research adoption. While architectures like nnU-Net \cite{isensee2021nnu} and vision foundation models have advanced biomedical segmentation, persistent challenges in cross-domain robustness and clinical translation continue to limit adoption, as evidenced by recent multi-modal validation studies \cite{isensee2024nnu}. The following subsections explicitly outline these challenges and the potential strategies currently employed to overcome them.
% }

% \textcolor{BulldogRed}{
\subsection{Generalizability and Transferability}
% }

% \textcolor{BulldogRed}{
Despite significant success, CNN-based segmentation models often exhibit poor generalizability and limited transferability across distinct biomedical imaging domains due to variability in imaging protocols, acquisition settings, and biological structures. CNN architectures like U-Net variants show excellent intra-domain performance but suffer catastrophic failure when applied cross-domain, as shown in multi-center MRI studies \cite{de2023domain}. These limitations arise largely because obtaining large, accurately labeled biomedical datasets is costly and time-intensive, often requiring substantial expert knowledge. As a consequence, researchers often design specialized deep learning models tailored specifically to individual imaging modalities or biological structures, limiting their broader application.
% }

% \textcolor{BulldogRed}{
Generalizability specifically denotes a model's capability to maintain robust performance on previously unseen datasets, distinct from the original training domain. This property is especially crucial in biomedical contexts, where significant variability arises due to diverse imaging protocols, acquisition devices, tissue characteristics, and patient-specific differences. Both data augmentation and transfer learning are widely utilized strategies to mitigate overfitting and significantly enhance generalization when training datasets are limited. Despite its proven effectiveness in limited-data scenarios, transfer learning introduces specific challenges, including domain discrepancies between source and target datasets, increased computational complexity, potential biases inherited from source domains, and persistent risks of overfitting.
% }

% \textcolor{BulldogRed}{
\textbf{Chapter \ref{ch:foundation}} addresses these limitations by evaluating the SAM \cite{kirillov2023segment} as a universal framework to bridge domain gaps, demonstrating how fine-tuning with biomedical-specific prompts enhances generalizability across modalities like MRI and fluorescence microscopy.
% }

% \textcolor{Hedges}{Get rid of these three sentences please!} Despite their success, CNN methods face challenges including poor generalizability, limited transferability, and the complexity of model development as well as fine-tuning pre-trained models in biomedical applications. This is due to the fact that manual labeling of data in biomedicine requires expert knowledge and is a costly and time-consuming task, making large and quality annotated datasets scarce. As a result, there exists a vast variety of deep learning models, each tailored to a specific modality and target structure. Generalizability refers to the trained model's ability to perform well on unseen data outside of the training set. It is a crucial aspect of machine learning, particularly in biomedical applications, where variations in image acquisition conditions, tissue types, and other factors can be substantial. Data augmentation techniques and transfer learning are two excellent methods to overcome overfitting and improve generalizability where the training data is small. While transfer learning is a powerful technique for leveraging pre-trained models to boost performance, especially in scenarios with limited data, it does come with its own set of challenges and limitations such as domain mismatch, risk of overfitting, computational demands, and potential biases from the source dataset.

% \textcolor{BulldogRed}{

\subsection{Data Scarcity and Annotation Strategies}
% }
% \textcolor{BulldogRed}{

In response to data scarcity and high annotation costs in biomedical segmentation tasks, multiple innovative solutions have emerged to leverage limited annotated data effectively. Semi-supervised learning \cite{van2020survey}, for instance, combines limited labeled datasets with abundant unlabeled data, effectively enhancing segmentation accuracy by exploiting underlying data distributions. Active learning \cite{settles2009active} strategically queries highly informative data points for expert annotation, optimizing labeling efforts and maximizing the value of limited annotated datasets. Data augmentation approaches, including geometric image transformations and synthetic image generation via Generative Adversarial Networks (GANs) \cite{goodfellow2014generative}, effectively increase data diversity, reduce overfitting, and substantially enhance model robustness.
% }

% \textcolor{BulldogRed}{
Transfer learning leverages knowledge gained from previously trained models on related tasks, substantially reducing the annotation burden for new segmentation applications. Self-supervised learning frameworks generate their supervisory signals via pretext tasks, such as predicting spatial positioning or reconstructing missing image regions \cite{chen2020simple}, thereby facilitating robust representation learning without explicit annotations. Emerging approaches, including few-shot, one-shot, and zero-shot learning \cite{zhao2023one}, specifically address extreme annotation scarcity by leveraging generalized representations, semantic metadata, or minimal annotated examples to effectively segment unseen biomedical structures.
% }

% \textcolor{BulldogRed}{
Chapters \ref{ch:cilia} and \ref{ch:contrastive} directly address these limitations: Chapter \ref{ch:cilia} introduces self-supervised pseudo-labels derived from optical flow to segment cilia without manual masks, while Chapter \ref{ch:contrastive} leverages contrastive learning to substantially reduce annotation dependence on Cell Tracking Challenge datasets \cite{mavska2023cell}.
% }

% \textcolor{Hedges}{Get rid of these three sentences please!} In the biomedical field, where labeled data is often scarce and costly to obtain, several solutions have been proposed to augment and utilize available data effectively. These include semi-supervised learning, which utilizes both labeled and unlabeled data to enhance learning accuracy by leveraging the data's underlying distribution. Active learning focuses on selectively querying the most informative data points for expert labeling, optimizing the training process by using the most valuable examples. Data augmentation techniques, such as image transformations and synthetic data generation through Generative Adversarial Networks, increase the diversity and volume of training data, enhancing model robustness and reducing overfitting. Transfer learning transfers knowledge from one task to another, minimizing the need for extensive labeled data in new tasks. Self-supervised learning creates its labels by defining a pretext task, like predicting the position of a randomly cropped image patch, aiding in the learning of useful data representations. Additionally, few-shot, one-shot, and zero-shot learning techniques are designed to operate with minimal or no labeled examples, relying on generalization capabilities or metadata for making predictions about unseen classes.

% \textcolor{BulldogRed}{
\subsection{Complementary Role of Unsupervised Methods}
% }
% \textcolor{BulldogRed}{
Unsupervised segmentation methods, which operate without annotated datasets or extensive pre-training, typically employ domain-specific heuristics or clustering techniques to delineate structures. Although these methods generally yield lower accuracy compared to supervised CNN-based methods, they offer significant advantages in reproducibility and generalizability due to their independence from prior data annotations and their minimal reliance on task-specific training \cite{ji2019invariant}. Consequently, these unsupervised approaches serve as complementary solutions, particularly beneficial in exploratory research contexts or resource-constrained scenarios.
% }

% \textcolor{BulldogRed}{
Chapter \ref{ch:cilia} exemplifies this approach, using unsupervised motion analysis to generate cilia segmentation masks, enabling reproducible phenotyping of dyskinetic ciliary motion without labeled training data \cite{vaezi2024training}.
% }

% \textcolor{Hedges}{Get rid of these three sentences please!}
% Unsupervised methods, on the other hand, do not require pre-training or an existing dataset and rely on domain-specific rules and heuristics. Although these methods exhibit less accuracy than CNN methods, they excel in reproducibility and generalizability as they do not depend on prior data knowledge. These different approaches to image segmentation provide a range of options for researchers to choose from, depending on their specific needs and resources.

% \textcolor{BulldogRed}{
\subsection{Reproducibility of Deep Learning Models}
% }
% \textcolor{BulldogRed}{
Reproducibility, the ability to consistently obtain similar outcomes from identical data and computational procedures, remains a critical requirement in biomedical research to validate findings and ensure reliable clinical translation. Ensuring reproducibility is fundamental to scientific integrity, enabling independent verification of results, building trust in methodologies, and facilitating reliable clinical and research translations. Reproducibility is heavily influenced by factors such as dataset variability, model architecture choices, optimization strategies, random initialization, and computational environments, each contributing potential sources of variability in outcomes \cite{maier2018rankings}.
% }

% \textcolor{BulldogRed}{
Lack of reproducibility not only undermines scientific validity but also results in significant resource waste, impeded scientific progress, misleading conclusions, and ethical implications, particularly critical in sensitive biomedical applications. Recent literature emphasizes comprehensive documentation, standardized model evaluation practices, fixed random seeds, robust cross-validation frameworks, explicit reporting of evaluation metrics, and open-source sharing of code and datasets to enhance reproducibility in biomedical deep learning research \cite{renard2020variability}.
% }

% \textcolor{BulldogRed}{
To promote reproducibility, this work open-sources code for TSeg in chapter \ref{ch:toxo} and contrastive learning pipelines in chapter \ref{ch:contrastive}, adopts fixed random seeds, and reports metrics like Dice scores with cross-validation across all experiments in chapters \ref{ch:toxo}, \ref{ch:cilia}, \ref{ch:contrastive}, and \ref{ch:foundation}, and shares relevant datasets publicly through Zenodo.
% }

% \textcolor{Hedges}{Get rid of these three sentences please!} Reproducibility refers to obtaining consistent results using the same input data, computational steps, methods, and conditions of analysis. Reproducibility is influenced by various factors including dataset variability, model architecture specifics, optimization procedures, and computational infrastructure. To ensure reproducibility in deep learning for medical image segmentation, Renard et al. advocate for comprehensive documentation, standardized practices, fixed random seeds, cross-validation, multiple evaluation metrics, and sharing of source code and dependencies. This concept is key in scientific research to ensure that outcomes can be reliably replicated under the same conditions, fostering trust and confidence in the findings. Apart from the loss of validity of a scientific method, non-reproducibility can lead to wasted resources, stalled scientific progress, erroneous conclusions, and significant ethical concerns.


% \textcolor{BulldogRed}{
\subsection{Practical Usability and Accessibility}
% }
% \textcolor{BulldogRed}{
While deep learning models exhibit strong performance across biomedical segmentation tasks, their practical usability remains contingent on multiple factors, including task complexity, data availability, and required model customization. For biomedical researchers and clinical practitioners with limited computational expertise, simplicity and ease of use become critical factors influencing adoption and effective utilization of deep learning-based segmentation tools \cite{wang2021annotation}. These users particularly benefit from segmentation methodologies that offer robust, out-of-the-box performance without extensive hyperparameter tuning or specialized configuration.
% }

% \textcolor{BulldogRed}{
Developing intuitive graphical user interfaces (GUIs) and interactive tools substantially improves accessibility for non-expert users, such as biologists and clinical practitioners, who require practical segmentation solutions without extensive technical knowledge. Facilitating easier interaction with deep learning models through user-friendly software not only enhances usability but also accelerates adoption and effective integration into routine clinical practice and biomedical research workflows \cite{sofroniew2022napari}.
% }

% \textcolor{BulldogRed}{
Chapters 2 and 4 operationalize usability through tools like TSeg's Napari plugin for no-code 3D segmentation and an interactive GUI for thresholding contrastive saliency maps, enabling biologists to deploy advanced segmentation without computational expertise.
% }

% \textcolor{Hedges}{Get rid of these three sentences please!} Deep learning models are effective across various applications but their usability depends on several factors such as the complexity of the task at hand, data availability, and the extent of necessary model customization. Incorporating an intuitive graphical user interface (GUI) and ensuring interactivity can enhance the usability of these tools, making them more accessible to non-expert users, such as biologists, who need practical, ready-to-use solutions without the intricacies of model adjustments. For users who prefer straightforward applications, ease of use is crucial. They benefit from methods that do not require extensive modifications or tuning to achieve optimal results.


% #############################################################################################
%   This is where your content will go. There will be examples of figures, tables, citations, and footnotes/sidenotes in this section. In the source files, there will be examples and how to get them to appear on the TOC, LOF, and LOTs Some Diciplines use section and subsection headings more than others. The Chapter one is really all that matters. This is strictly to give the reader a sence of how they look on the TOC.
%   \section{Your first section}
%     How do you feel so far? In these next few subsections we are going to be illustrating how to do these different things in LaTeX. Hopefully you will be able to replicate it whenever you need them.
%     It is good to have all of you images (figures) in their own folder so you can keep some organization to your projects.

%     \subsection{Table Example}
%       In this subsection, we are going to give an example of a table and how one will look. There are many ways to make a table and customize them. Here is one example:


%     \begin{table}[ht]
%       \centering

%     \begin{tabular}[c]{|l|c|rc|}
%       \hline
%       left justified  & centerd  & right justified  & no left border\\
%       \hline
%       row 1 & fill & in between & the \&s\\
%       row 2 & & &\\
%       \hline
%     \end{tabular}
%     \caption{The Caption of the table.}
%     \label{table:someTable}
%     \end{table}



%     \subsection{Figure Example}
%     This is going to be an example of how to insert an image and it's caption. We can also reference it anywhere else in the document as well.
%     \begin{figure}[h]
%       \centering\includegraphics[width=0.5\textwidth]{figures/digilab_logo}
%       \caption{The Digital Humanities Lab Logo}
%       \label{fig:digilogo}
%     \end{figure}
%     The Digital Humanities logo, figure ~\ref{fig:digilogo} is one of many different prototypes.

%     \subsection{Equation Examples}
%       You can also reference an equation just like a figure and table. There are two different enviornments that are needed. The first is just using the equation enviornment. This is useful when you have one equation to write. As seen here:
%         \begin{equation*}
%           Y=\beta_{0} + \sum\limits_{i=1}^n \beta_{i}X_{i} + e
%         \end{equation*}
% The equation for the general multiple linear regression model is above. This is a good use case for the equation enviornment. \LaTeX  knows to expect math symbols inside the enviornment. What if you need more than one line? What if each line need to align? Thats when the align enviornment comes in handy. A lot of the times, you will need to show the simplification of equations or steps in calcualtions.
% \begin{center}
%   \begin{align*}
%     P_{s}                & = \frac{D*F}{N*P*I}                         \\
%     \\
%                          & = \frac{D*R*W}{N*P*I} \text{  since } F=R*W \\
%     \\
%     0.70                 & = \frac{1*R*0.1}{2.5*1*4,294,967,295}       \\
%     \\
%     R                    & = 751619276625.0                            \\
%     \\
%     A=\frac{R*0.1}{4096} & \implies A = 1835007.99
%   \end{align*}
% \end{center}

\end{document}
