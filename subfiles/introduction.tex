% !TEX root =../dissertation.tex
\documentclass[./dissertation.tex]{subfiles}
\begin{document}




\chapter{Introduction}



\textcolor{BulldogRed}{Biomedical image segmentation is critical for accurate diagnosis, treatment planning, and biological research, as it precisely delineates anatomical structures and pathological changes. Accurate segmentation enables quantitative analyses, enhancing objectivity, consistency, and reproducibility in medical assessments. Historically, image segmentation relied on traditional methods such as thresholding, watershed segmentation, and optical flow, primarily utilizing pixel intensity to differentiate image regions. However, these traditional techniques frequently encountered significant difficulties when applied to noisy imaging conditions, complex anatomical structures, or subtle intensity gradients, severely limiting their practical applicability in clinical and research settings. Addressing these limitations necessitated sophisticated computational approaches, driving substantial methodological evolution toward advanced image segmentation strategies.} \textcolor{Olympic}{Get rid of this sentence please!} Image segmentation started with traditional methods and algorithms such as thresholding, watershed, and optical flow, which rely on pixel intensity.

\textcolor{BulldogRed}{Traditional rule-based segmentation methods, including thresholding and watershed algorithms, operate by direct pixel intensity analysis to define boundaries but exhibit limited robustness in challenging imaging conditions characterized by noise or poor contrast. Machine learning-based segmentation techniques, such as Support Vector Machines (SVMs), Random Forest classifiers, and contrastive learning methods, subsequently enhanced segmentation capabilities through statistical pattern recognition. These methods provided improved flexibility and accuracy by learning complex data patterns from annotated examples. Despite these advancements, traditional machine learning methods required intensive manual feature engineering, constraining scalability and limiting generalization across diverse biomedical datasets and modalities. The advent of deep learning approaches effectively addressed these limitations, signifying a transformative shift in image segmentation research.} \textcolor{Olympic}{Get rid of this two sentences please!} Traditional rule-based methods like Thresholding and Watershed analyze pixel values to identify borders and boundaries within areas of interest. Machine learning-based methods - such as Support Vector Machines (SVM), random forests, and contrastive learning - involve the use of statistical machine learning models and increased the popularity and applicability of segmentation.

\textcolor{BulldogRed}{Deep learning-based segmentation methods, particularly those leveraging Convolutional Neural Networks (CNNs), revolutionized the field by automatically learning hierarchical feature representations directly from raw images, thus eliminating the need for manual feature extraction. CNNs inherently learn both low-level spatial features and high-level semantic features, significantly enhancing segmentation accuracy, consistency, and robustness across diverse biomedical datasets. These deep learning-based segmentation techniques typically fall into supervised, semi-supervised, and unsupervised categories, differentiated by the amount and type of annotated data required. Among supervised methods, the U-Net architecture, specifically developed for biomedical image segmentation, has emerged as a cornerstone due to its highly effective feature extraction and superior generalization capabilities, setting foundational benchmarks for current segmentation research.} \textcolor{Olympic}{Get rid of these four sentences please!} Finally, deep learning-based methods leverage neural networks to learn hierarchical feature representation from raw images without requiring manual feature engineering. This process saw significant improvement with the introduction of Convolutional Neural Networks (CNN). CNNs are trained to detect features in regions of interest, enabling them to perform similarly on new images. Segmentation techniques can be divided into three categories: supervised, semi-supervised, and unsupervised.

\textcolor{BulldogRed}{U-Net and its variants have achieved widespread adoption in biomedical image segmentation due to their exceptional ability to automate feature extraction directly from images without manual intervention or extensive preprocessing. Variants include foundational architectures such as the original U-Net and 3D U-Net, specialized for volumetric data segmentation in computed tomography (CT) and magnetic resonance imaging (MRI). Advanced variants introduce architectural enhancements, including Attention U-Net for selective feature refinement, Inception U-Net for multi-scale context capture, Residual U-Net for deeper training capabilities, and Dense U-Net promoting extensive feature reuse to enhance accuracy and training efficiency. Collectively, these CNN-based architectures currently represent the state-of-the-art in biomedical image segmentation, significantly advancing diagnostic precision and enabling more reliable quantitative analyses in medical and biological research.} \textcolor{Olympic}{Get rid of these seven sentences please!} U-Net and its variants are extensively adopted in biomedical image segmentation for their capability to automatically extract features from images without manual intervention or preprocessing. They can learn high-level semantic information and low-level spatial information from large-scale data. U-Net architectures are categorized based on their design and functionality. Basic U-Nets, like the original U-Net and 3D U-Net, are foundational, with the latter extending to 3D data for volumetric segmentation useful in CT and MRI scans. Advanced U-Nets include Attention U-Net, which uses attention mechanisms for precision; Inception U-Net, capturing multi-scale information through varied kernel sizes; Residual U-Net, which incorporates residual connections to aid deep network training; and Dense U-Net, promoting feature reuse via dense connections. Currently, CNNs represent the state-of-the-art in image segmentation, with U-Net being the predominant architecture, especially in the field of biomedical image segmentation. These advancements have greatly improved the accuracy and efficiency of biomedical image analysis, making it an essential tool in various fields of research.

\textcolor{BulldogRed}{Biomedical images span diverse imaging modalities, each presenting unique segmentation challenges due to varying image characteristics, resolution, noise levels, and structural complexity. Clinical imaging modalities, including CT, MRI, positron emission tomography (PET), and ultrasound imaging, each feature distinct properties affecting segmentation, such as variable tissue contrast, noise artifacts, and differing spatial resolutions. Microscopy imaging modalities, such as fluorescence microscopy, bright-field microscopy, phase-contrast microscopy, and electron microscopy, present unique challenges, including intricate subcellular structures, variable staining methods, and diverse cellular morphologies. Given the extensive biological diversity, segmentation targets range widely from microscopic features like nuclei, mitochondria, and cilia to macroscopic anatomical structures like tumors, lesions, blood vessels, bones, brain anatomy. This combination of biological variability and modality-specific imaging challenges underscores the necessity for specialized yet adaptable deep learning models capable of efficiently generalizing across multiple biomedical segmentation tasks without extensive reconfiguration.} \textcolor{Olympic}{Get rid of these four sentences please!} Biomedical images come in a vast variety of formats, types, and modalities. The modalities in medical imaging include computed tomography (CT), magnetic resonance imaging (MRI), positron emission tomography (PET), and ultrasound, while microscopy modalities include fluorescent microscopy, bright-field, lens-free microscopy, light microscopy, volume electron microscopy, and phase contrast microscopy, just to name a few. Similarly, due to the variety of biological structures, segmentation targets can vary from nuclei and cell membranes to organelles such as mitochondria, cilia, tumors, and lesions, as well as blood vessels, bone, and brain structures. This diversity in imaging techniques and segmentation targets highlights the need for specialized and customizable deep learning models in biomedical applications.

\textcolor{BulldogRed}{Recently, universal segmentation frameworks, exemplified by the Segment Anything Model (SAM), have emerged as significant advancements aiming to generalize segmentation tasks across diverse imaging domains. SAM leverages flexible user inputs, including single points, bounding boxes, and textual descriptions, enabling robust segmentation performance without requiring specialized retraining or extensive annotated datasets. Its effectiveness arises from a transformer-based architecture combined with extensive pre-training on large-scale, diverse image datasets, facilitating segmentation even of previously unseen biomedical structures and modalities, thus significantly reducing the practical barriers for broad biomedical adoption.} \textcolor{Olympic}{Get rid of these three sentences please!} The Segment Anything Model (SAM) can segment an object within an image using user inputs, including a single point, multiple points, an entire mask, a bounding box, or textual descriptions. This functionality is based on the modelâ€™s inherent ability to recognize objects, which enables it to segment unfamiliar object types without further training, effectively supporting zero-shot learning. Furthermore, the effectiveness of SAM is enhanced by its specialized architecture and the use of a significantly large dataset.

\textcolor{BulldogRed}{The central objective of this dissertation is to systematically address existing gaps in biomedical image segmentation methodologies, explicitly focusing on enhancing model generalizability, reducing dependency on annotated datasets, and improving usability for practical biomedical applications. Specifically, the research investigates three interconnected questions: (1) How can supervised segmentation methods be optimized to enhance performance and generalization across diverse biomedical datasets? (2) In what ways can unsupervised and self-supervised learning techniques effectively reduce the need for extensive annotated biomedical datasets without compromising segmentation accuracy? (3) How can universal segmentation frameworks be adapted and validated for robust performance in diverse biomedical imaging scenarios?}

\textcolor{BulldogRed}{The dissertation is organized to comprehensively address these research questions through a structured approach. Chapter 2 rigorously evaluates supervised deep learning segmentation methods, emphasizing performance optimization, generalization improvement, and methodological enhancements across diverse biomedical imaging datasets. Chapter 3 explores unsupervised and self-supervised segmentation approaches, explicitly focusing on methods that reduce reliance on annotated data, critically evaluating their effectiveness and generalizability. Chapter 4 focuses on adapting and validating universal segmentation frameworks such as SAM, investigating their applicability, robustness, and performance across various biomedical modalities and segmentation targets. Finally, Chapter 5 synthesizes these findings, discusses implications, and suggests directions for future biomedical segmentation research.}

\textcolor{BulldogRed}{Despite these methodological advancements, significant barriers persist, hindering wide-scale adoption of deep learning segmentation methods in biomedical imaging. These ongoing challenges include limited model generalizability, heavy reliance on extensive annotated datasets, poor transferability across distinct biomedical imaging modalities, and practical usability constraints, especially for users lacking extensive computational expertise. The following "Challenges" section delves deeper into these issues, setting the stage for subsequent chapters, which systematically propose, develop, and evaluate novel solutions addressing these critical barriers.}

\section{Challenges}
Despite their success, CNN methods face challenges including poor generalizability, limited transferability, and the complexity of model development as well as fine-tuning pre-trained models in biomedical applications. This is due to the fact that manual labeling of data in biomedicine requires expert knowledge and is a costly and time-consuming task, making large and quality annotated datasets scarce. As a result, there exists a vast variety of deep learning models, each tailored to a specific modality and target structure. Unsupervised methods, on the other hand, do not require pre-training or an existing dataset and rely on domain-specific rules and heuristics. Although these methods exhibit less accuracy than CNN methods, they excel in reproducibility and generalizability as they do not depend on prior data knowledge. These different approaches to image segmentation provide a range of options for researchers to choose from, depending on their specific needs and resources.

In the biomedical field, where labeled data is often scarce and costly to obtain, several solutions have been proposed to augment and utilize available data effectively. These include semi-supervised learning, which utilizes both labeled and unlabeled data to enhance learning accuracy by leveraging the dataâ€™s underlying distribution. Active learning focuses on selectively querying the most informative data points for expert labeling, optimizing the training process by using the most valuable examples. Data augmentation techniques, such as image transformations and synthetic data generation through Generative Adversarial Networks, increase the diversity and volume of training data, enhancing model robustness and reducing overfitting. Transfer learning transfers knowledge from one task to another, minimizing the need for extensive labeled data in new tasks. Self-supervised learning creates its labels by defining a pretext task, like predicting the position of a randomly cropped image patch, aiding in the learning of useful data representations. Additionally, few-shot, one-shot, and zero-shot learning techniques are designed to operate with minimal or no labeled examples, relying on generalization capabilities or metadata for making predictions about unseen classes.

Generalizability refers to the trained modelâ€™s ability to perform well on unseen data outside of the training set. It is a crucial aspect of machine learning, particularly in biomedical applications, where variations in image acquisition conditions, tissue types, and other factors can be substantial. Data augmentation techniques and transfer learning are two excellent methods to overcome overfitting and improve generalizability where the training data is small. While transfer learning is a powerful technique for leveraging pre-trained models to boost performance, especially in scenarios with limited data, it does come with its own set of challenges and limitations such as domain mismatch, risk of overfitting, computational demands, and potential biases from the source dataset.

Reproducibility refers to obtaining consistent results using the same input data, computational steps, methods, and conditions of analysis. This concept is key in scientific research to ensure that outcomes can be reliably replicated under the same conditions, fostering trust and confidence in the findings. Reproducibility is influenced by various factors including dataset variability, model architecture specifics, optimization procedures, and computational infrastructure. Apart from the loss of validity of a scientific method, non-reproducibility can lead to wasted resources, stalled scientific progress, erroneous conclusions, and significant ethical concerns. To ensure reproducibility in deep learning for medical image segmentation, Renard et al. advocate for comprehensive documentation, standardized practices, fixed random seeds, cross-validation, multiple evaluation metrics, and sharing of source code and dependencies.

Deep learning models are effective across various applications but their usability depends on several factors such as the complexity of the task at hand, data availability, and the extent of necessary model customization. For users who prefer straightforward applications, ease of use is crucial. They benefit from methods that do not require extensive modifications or tuning to achieve optimal results. Incorporating an intuitive graphical user interface (GUI) and ensuring interactivity can enhance the usability of these tools, making them more accessible to non-expert users, such as biologists, who need practical, ready-to-use solutions without the intricacies of model adjustments.











% #############################################################################################
%   This is where your content will go. There will be examples of figures, tables, citations, and footnotes/sidenotes in this section. In the source files, there will be examples and how to get them to appear on the TOC, LOF, and LOTs Some Diciplines use section and subsection headings more than others. The Chapter one is really all that matters. This is strictly to give the reader a sence of how they look on the TOC.
%   \section{Your first section}
%     How do you feel so far? In these next few subsections we are going to be illustrating how to do these different things in LaTeX. Hopefully you will be able to replicate it whenever you need them.
%     It is good to have all of you images (figures) in their own folder so you can keep some organization to your projects.

%     \subsection{Table Example}
%       In this subsection, we are going to give an example of a table and how one will look. There are many ways to make a table and customize them. Here is one example:


%     \begin{table}[ht]
%       \centering

%     \begin{tabular}[c]{|l|c|rc|}
%       \hline
%       left justified  & centerd  & right justified  & no left border\\
%       \hline
%       row 1 & fill & in between & the \&s\\
%       row 2 & & &\\
%       \hline
%     \end{tabular}
%     \caption{The Caption of the table.}
%     \label{table:someTable}
%     \end{table}



%     \subsection{Figure Example}
%     This is going to be an example of how to insert an image and it's caption. We can also reference it anywhere else in the document as well.
%     \begin{figure}[h]
%       \centering\includegraphics[width=0.5\textwidth]{figures/digilab_logo}
%       \caption{The Digital Humanities Lab Logo}
%       \label{fig:digilogo}
%     \end{figure}
%     The Digital Humanities logo, figure ~\ref{fig:digilogo} is one of many different prototypes.

%     \subsection{Equation Examples}
%       You can also reference an equation just like a figure and table. There are two different enviornments that are needed. The first is just using the equation enviornment. This is useful when you have one equation to write. As seen here:
%         \begin{equation*}
%           Y=\beta_{0} + \sum\limits_{i=1}^n \beta_{i}X_{i} + e
%         \end{equation*}
% The equation for the general multiple linear regression model is above. This is a good use case for the equation enviornment. \LaTeX  knows to expect math symbols inside the enviornment. What if you need more than one line? What if each line need to align? Thats when the align enviornment comes in handy. A lot of the times, you will need to show the simplification of equations or steps in calcualtions.
% \begin{center}
%   \begin{align*}
%     P_{s}                & = \frac{D*F}{N*P*I}                         \\
%     \\
%                          & = \frac{D*R*W}{N*P*I} \text{  since } F=R*W \\
%     \\
%     0.70                 & = \frac{1*R*0.1}{2.5*1*4,294,967,295}       \\
%     \\
%     R                    & = 751619276625.0                            \\
%     \\
%     A=\frac{R*0.1}{4096} & \implies A = 1835007.99
%   \end{align*}
% \end{center}

\end{document}
