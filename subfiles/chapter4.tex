% !TEX root =../dissertation.tex
\documentclass[./dissertation.tex]{subfiles}
\begin{document}
\chapter{Toward a Foundation Model for Biomedical Image Segmentation}
\label{ch:foundation}
\section{Introduction}

% 1. Supervised methods require large annotated data to perform well. Their generalizability suffers. \\
% 2. Unsupervised methods are good in that they don't need a lot of annotated data, but their accuracy is not as good and they can be very domain-specific. \\

Biomedical image segmentation is pivotal across a diverse array of medical and biological applications, from diagnostic imaging to cellular analysis. While supervised segmentation methods, especially convolutional neural networks (CNNs), have achieved remarkable accuracy, their reliance on large, annotated datasets severely limits their generalizability and transferability, particularly in biomedical contexts where labeled data is scarce and costly to obtain. Conversely, unsupervised methods, despite being simpler and more generalizable, often fall short in segmentation precision and robustness, thus failing to meet the accuracy requirements of clinical and research settings.


% 3. foundation models are trained on very large datasets. SAMs are good at zero-shot segmentation for they learn the repesentation of textures, objects, etc. they can take in prompts in the form of bounding boxes and points. these SAMs can potentially be a good easy to use choice for producing good-enough masks. \\
% 4. How SAMs work. the internal mechanism that allows it to take points and generage masks. or just generate masks for all objects in the image. \\
% 5. SAM struggles with biomedical images since it is originally trained on general images (e.g. horse, cat, cat, etc.). that's why it needs to be fine-tuned to be able to perform reliably for biomedical images. \\

Recently, the emergence of Foundation Models (FMs) and the Segment Anything Model (SAM) offers a compelling new direction for addressing these limitations. SAM, introduced by Kirillov et al. \cite{kirillov2023segment}, marked a significant advancement by achieving impressive zero-shot segmentation capabilities across varied image domains, relying only on minimal user prompts such as points or bounding boxes. Its successor, SAM 2, further extends these capabilities into video segmentation, leveraging advanced architectures to improve accuracy and interaction efficiency \cite{ravi2024sam}. These models are trained on large and diverse datasets, enabling them to generalize effectively across multiple segmentation tasks without extensive domain-specific training. Despite their potential, the application of general-purpose SAM models to biomedical imaging presents unique challenges. The complex and nuanced nature of biomedical images characterized by varying imaging modalities, structures, textures, and noise levels means that models trained primarily on general-domain images may struggle to achieve desirable segmentation accuracy \cite{mazurowski2023segment,na2024segment}. Recognizing this, adaptations of SAM tailored specifically to biomedical contexts have emerged. Models such as MedSAM \cite{ma2024segment} and MediViSTA-SAM \cite{kim2025medivista} demonstrate the feasibility of adapting SAM to medical imaging and video analysis, showing promising results that often surpass specialized, modality-specific models in robustness and accuracy.



% 6. There are many biomedical adaptations for SAM, including this and that and the other one which do this and that and something else, each of which focusing on improving some aspect of SAM. They all show that their work has promising improvements. \\

% 7. There's another set of research done on incorporating text prompts into SAM. it involves training text-image pairs first and the model uses contrastive learning to clusterize or localize the similar text-image embeddings. CLIP is a famous model. in inferrence, this CLIP model takes the prompt and image and tries to highlight the locations on the image which it finds relevant to the prompt. then some random points or bounding boxes are created using this heatmap and then passed to SAM for downstream segmentation. there are of course some biomedical adaptations of CLIP like BiomedCLIP. they perform great vs. the vanilla CLIP. \\


% 8. But there is so much stochasticity and randomness within this process. from the way the CLIP model is trained or fine-tuned to how this inference is done and how the point and box prompts are selected. they all affect the final outcome of the SAM's mask decoder. \\

Alongside spatial segmentation capabilities, the integration of vision-language models like BiomedCLIP \cite{zhang2023biomedclip} into SAM workflows has gained considerable attention. BiomedCLIP, trained on extensive biomedical image-text pairs, provides robust multimodal embeddings that bridge textual and visual domains, enabling powerful text-driven segmentation. Frameworks such as MedCLIP-SAMv2 exemplify this integration, demonstrating how textual prompts can effectively guide precise segmentation tasks, from identifying tumors in medical scans to delineating specific cellular structures in microscopy images. However, the integration of vision-language models with SAM introduces additional layers of complexity and stochasticity. Variability in outputs stemming from factors such as the fine-tuning process of models like BiomedCLIP, differences in textual prompts, or randomness in inference strategies can undermine reproducibility, a critical factor in biomedical research and clinical applications. Thus, understanding and mitigating this stochasticity is paramount for the practical adoption and reliability of these models.

% 9. that's why we're focusing on this part, aiming to find ways to control this randomness and increase the reproducibility of the CLIP, hence helping SAM in honing in on the ROI that best matches the prompt.

This chapter aims to comprehensively explore and address these challenges. We focus specifically on evaluating and enhancing the reproducibility of integrated vision-language segmentation models, investigating how different factors—including fine-tuning strategies, prompt engineering, and inference methodologies—influence variability in segmentation outcomes. Our goal is to develop methodologies that standardize and optimize these variables, ensuring SAM-based models are not only accurate and versatile but also reliably reproducible in biomedical contexts.

\section{Background}

% 1. SAM's original paper, and the subsequent SAM2. \\

SAM \cite{kirillov2023segment} introduced a groundbreaking promptable segmentation approach, trained on a vast dataset (SA-1B), featuring over one billion masks. Its architecture comprises three components: a powerful Vision Transformer (ViT) image encoder, a flexible prompt encoder handling points, boxes, and text, and a lightweight mask decoder to produce segmentation masks. SAM's notable innovation lies in its zero-shot capabilities achieved through prompt engineering, enabling it to generalize well across various segmentation tasks, even without task-specific fine-tuning. SAM-2 \cite{ravi2024sam} extends SAM's capabilities to video data, incorporating a memory attention mechanism that retains information from previous frames, thereby significantly enhancing segmentation accuracy and interaction efficiency. SAM-2 utilizes a hierarchical transformer architecture (Hiera) \cite{ryali2023hiera,bolya2023window} pre-trained with masked autoencoders (MAE) \cite{he2022masked}, making it highly effective for real-time segmentation tasks across images and videos. This memory-enhanced architecture allows SAM-2 to iteratively refine masks, leading to considerable improvements in temporal segmentation consistency. A comprehensive survey titled "Foundation Models for Biomedical Image Segmentation" \cite{lee2024foundation} underscores the transformative potential of SAM, summarizing over 100 studies that have successfully adapted SAM to a wide range of biomedical datasets. The survey highlights SAM's strong zero-shot capabilities and outlines various domain-specific tuning methods and data scarcity challenges that have driven innovation in biomedical segmentation.


% 2. Biomedical adaptations of SAM and what they focus on. \\

The success of the Segment Anything Model (SAM) in general-domain image segmentation has inspired adaptations and methodological enhancements aimed at tailoring its capabilities specifically for biomedical applications, addressing unique challenges associated with medical image segmentation. MedSAM \cite{ma2024segment} and other models, like BioSAM-2 \cite{yan2024biomedical}, have demonstrated the necessity of domain-specific fine-tuning for achieving clinical-grade accuracy. BioSAM-2, particularly designed for biomedical segmentation, has optimized SAM-2 with medical domain-specific data and additional memory mechanisms for improved performance across diverse biomedical imaging modalities. Medical SAM Adapter (Med-SA) \cite{wu2023medical} introduced adaptation modules such as Space-Depth Transpose (SD-Trans) and Hyper-Prompting Adapter (HyP-Adpt), which enhance SAM's performance on medical images through minimal yet strategic parameter adjustments. These modules have shown significant improvements over traditional segmentation methods by efficiently incorporating medical domain knowledge.

In the realm of prompt learning and auto-prompting the Segment Any Cell (SAC) \cite{na2024segment} framework leveraged auto-prompting and fine-tuning methods, using Low-Rank Adaptation (LoRA) \cite{hu2022lora}, to automatically generate effective prompts for nuclei segmentation. This method reduced manual intervention and improved segmentation accuracy in microscopic imaging scenarios. SSPrompt \cite{huang2024learning} optimized SAM's spatial and semantic prompts directly within its embedding space, enhancing its generalization capabilities across complex segmentation tasks. The Segment and Caption Anything \cite{huang2024segment} model enriched SAM's semantic understanding capabilities by integrating a query-based feature mixer, improving semantic precision and enabling the model to provide meaningful regional captions, thus enhancing segmentation results through better semantic contextualization.



% 3. Biomedical adaptations of CLIP. some researches matched the best description to the image. some other did other things. \\

% Incorporation and Integration of Text Prompts in Biomedical SAM
% a. Vision-Language Models and Multi-Modal Integration
The support for textual prompts in the original SAM is relatively limited and experimental compared to spatial (points, boxes, masks) prompts. Building upon the advancements in adapting SAM for biomedical tasks, recent research has increasingly focused on incorporating text prompts and integrating vision-language models to further enhance segmentation precision and semantic interpretability in medical imaging. Models like BiomedCLIP \cite{zhang2023biomedclip} and adaptations such as MedCLIP-SAMv2 \cite{koleilat2024medclipsamv2, koleilat2024medclip} underscore the importance of text-driven segmentation approaches, leveraging extensive biomedical image-text pairs to provide robust multimodal embeddings. This integration enables powerful and precise segmentation guided by textual descriptions, thus bridging visual and textual biomedical data effectively. The EVF-SAM \cite{zhang2024evf} model exemplifies the integration of early vision-language fusion. It incorporates an early fusion mechanism, significantly outperforming late fusion models by enhancing text-to-image attention, which is critical for accurate segmentation guided by referring expressions.

% b. Specific Applications and Advancements in Text-guided Biomedical Segmentation
Polyp-SAM++ \cite{biswas2023polyp} demonstrated the effectiveness of detailed textual prompts specifically for colorectal polyp segmentation, showing how text guidance could substantially improve the segmentation accuracy and robustness of SAM, particularly in clinically relevant contexts . Hi-SAM \cite{ye2024hi} extended SAM's capabilities to hierarchical text segmentation, including pixel-level text, word, text-line, and paragraph segmentation, thus enabling more structured and detailed biomedical image analyses, crucial for applications like pathology slide examination. PROMISE \cite{li2024promise} and similar models have adapted SAM to 3D biomedical segmentation, introducing lightweight adapters for depth-related spatial context and achieving superior performance in tumor segmentation tasks by effectively combining textual prompts with depth-awareness.

% 4. Universal Segmentation and Multi-Modal Integration for Enhanced Biomedical Analysis
The Segment Anything with Text prompts (SAT) \cite{zhao2023one} model, trained on an extensive dataset comprising over 22,000 medical scans and nearly 500 anatomical classes, exemplifies a universal segmentation framework that integrates extensive medical terminologies as textual prompts. This approach emphasizes the utility of incorporating domain-specific knowledge directly into the model training, significantly improving segmentation performance across diverse medical imaging tasks.

When using text-to-segmentation pipelines, segmentation results should ideally be deterministic given the same input. However, randomness can creep in through various stages, both at inference and during training. There is growing interest in modeling the stochasticity and uncertainty inherent in medical image segmentation. In practice, what constitutes the "correct" segmentation can be ambiguous – different experts may trace slightly different boundaries for the same lesion, especially in low-contrast or complex cases \cite{rakic2024tyche}. Models like SAM produce one deterministic mask per prompt, which doesn't capture this ambiguity, however, this ambiguity is maximised when incorporating text-to-segment pipelines, since the text-image input of the CLIP should be transformed into spacial points to be fed to SAM as input prompts. In MedCLIP-SAMv2 \cite{koleilat2024medclip} this step is done with the help of extracting attention maps of BiomedCLIP given a text-image pair. This saliency map highlights the locations of interest in the image which are then used to select points or bounding boxes for SAM.

% In this article we take MedCLIP-SAMv2 to be the ground of our research and attempt to understand the impacting factors and parameters to the reproducibility of the attention maps of the text-image encoder. We will investigate how effective each of these 

Our methodology builds upon the principles of MedCLIP-SAMv2, integrating the BiomedCLIP vision-language model with SAM to improve segmentation accuracy and reproducibility in biomedical applications. Initially, we evaluate the baseline capability of BiomedCLIP to differentiate various biological structures, emphasizing its performance on complex and previously unseen entities such as ciliary regions. Subsequently, we explore the impact of fine-tuning BiomedCLIP, examining how variations in fine-tuning parameters influence its effectiveness. Lastly, we assess multiple strategies for selecting optimal spatial prompts as inputs for SAM, aiming to identify methods that consistently yield accurate segmentation outcomes.
%  specifically in detecting ciliary regions from nasal epithelial biopsy videos. 

\section{Methodology}
% 1. my methodology involves fine-tuning BiomedCLIP on the dataset of cilia i have. the cilia dataset contains videos or nasal epithelial biopsy containing ciliary regions. these videos are annotated and the annotations show where the body of the cell is and where the ciliary region are. sometimes the ciliary region sticks out of the boundary of the cell which makes them easy to detect with eye and some other times they overlap the body of the cell or are out of focus which makes them very hard to detect even by human. anyway. I finetune the BiomedCLIP model using masked images of cell bodies and masked images of cilia along with their respective textual descriptions and then use the model to infer on unseen data. my aim is to find out which factors undermine the reproducibility of this process. those are: fine-tuning the model and for how long and how much, the text descriptions of the patches, whether using masked images or raw images, and - finally when the BiomedCLIP spits out a heatmap of its predictions - what's the best strategy in selecting positive/negative points or bounding boxes considering the output of the BiomedCLIP - that is going to be the input of SAM - to make sure we get accurate and reproducible results

This chapter outlines a systematic approach developed to enhance segmentation accuracy and reproducibility in biomedical imaging, specifically targeting the segmentation of ciliary regions in nasal epithelial biopsy videos. Leveraging the BiomedCLIP vision-language model integrated with the Segment Anything Model (SAM), the methodology involves fine-tuning BiomedCLIP on a specialized dataset containing annotated videos of nasal epithelial biopsies. These annotations distinctly mark cell bodies and associated ciliary regions, which vary significantly in visibility—ranging from clearly delineated, easily identifiable structures to overlapping and out-of-focus regions challenging even to expert human annotators. The methodology rigorously explores how different fine-tuning parameters, textual prompts, and image pre-processing strategies (masked versus raw) influence BiomedCLIP's performance. Subsequently, the trained BiomedCLIP generates predictive heatmaps on previously unseen data, serving as a basis for strategically selecting spatial prompts for SAM segmentation. Finally, this research investigates various prompt selection strategies to determine optimal methods for ensuring consistent, accurate, and reproducible segmentation results.



\subsection{Datasets}

\subsubsection{Cilia Dataset}
Our principal dataset comprises 681 microscopy videos of nasal epithelial biopsies, among which 325 videos have detailed annotations. Each video depicts epithelial cells exhibiting either normal motile cilia, dyskinetic (immotile) cilia. Annotated masks explicitly identify three types of regions: cell bodies, clearly visible cilia structures, and overlapping, hard-to-detect cilia. Visible cilia typically extend beyond cell boundaries, appearing clearly against a blank background, thus facilitating their straightforward identification in static frames. Conversely, overlapping cilia structures, often oriented vertically toward the microscope lens or appearing sparse and out of focus, pose significant detection challenges, even to human annotators. Such challenging structures usually require observing subtle rhythmic patterns across video frames for confident identification.

For simplicity and consistency within our methodology, both visible and overlapping ciliary regions were grouped into a single class termed "ciliary structure." To evaluate the effectiveness of fine-tuning BiomedCLIP and subsequent segmentation using SAM, the annotated cilia dataset was split into training and testing subsets following a 70/30 ratio.
% The cilia dataset comprises annotated videos highlighting cell bodies and ciliary regions, which vary significantly in visibility and overlap, presenting unique detection challenges.

\subsubsection{The Cell Tracking Challenge Dataset \cite{mavska2023cell}:}
We employed selected datasets from the Cell Tracking Challenge (CTC), which offers a comprehensive collection of 2D and 3D time-lapse microscopy images, each representing diverse biological organisms and imaging modalities. These datasets cover a broad range of specimens, including human, mouse, rat, Caenorhabditis elegans, Drosophila melanogaster, and others, captured using modalities such as Brightfield, Differential Interference Contrast (DIC), and Fluorescence microscopy (detailed in Table 2.1). Given the relative uniformity of image slices within individual datasets, a small representative sample from each was sufficient for benchmarking BiomedCLIP's inherent segmentation capabilities prior to fine-tuning.

\subsubsection{BiomedCLIP Model}
% BiomedCLIP has been combined with SAM in models like MedCLIP-SAMv2, where BiomedCLIP generates saliency maps or text-based feature maps that guide SAM’s segmentation process. This integration allows text-driven segmentation (e.g., segmenting "tumor region" based on a natural language prompt), opening doors to more interactive and automated medical image analysis. BiomedCLIP is a multimodal foundation model designed for biomedical image and text understanding. It is a contrastive vision-language model trained on 15 million biomedical image-text pairs extracted from PubMed Central. The model learns joint representations of biomedical images and their textual descriptions, enabling it to perform various medical vision-language tasks.
% BiomedCLIP is based on OpenAI’s CLIP (Contrastive Language–Image Pretraining) but adapted for biomedical data. Its training follows the standard CLIP contrastive learning pipeline:

% Image encoder: A vision transformer (ViT-B/16 or ViT-L/14) processes images.
% Text encoder: A transformer-based language model (PubMedBERT) encodes corresponding biomedical text descriptions.
% Contrastive learning: The model learns to map similar images and text closer in embedding space while pushing unrelated pairs apart.
% Unlike vanilla CLIP, which was trained on general images (e.g., cats, landscapes, objects), BiomedCLIP is trained specifically on biomedical data, allowing it to capture domain-specific features crucial for tasks like radiology, histopathology, and medical illustrations.

BiomedCLIP, a contrastive vision-language model tailored specifically for biomedical domains, integrates effectively with segmentation frameworks like SAM, exemplified by models such as MedCLIP-SAMv2. Trained on 15 million biomedical image-text pairs from PubMed Central, BiomedCLIP leverages a vision transformer (ViT) for image encoding and PubMedBERT \cite{gu2021domain} for text encoding. Through contrastive learning, it aligns related images and textual descriptions into a joint embedding space. Unlike general-purpose models like CLIP \cite{radford2021learning}, BiomedCLIP captures nuanced, domain-specific features, making it uniquely suited for precise biomedical image analysis tasks, including saliency-driven segmentation.

\begin{figure}[h]
    \centering\includegraphics[width=.48\textwidth]{figures/sam/untuned1.png}
    \centering\includegraphics[width=.48\textwidth]{figures/sam/untuned2.png}
    \caption{BiomedCLIP's out-of-the-box performance over CTC's 2D datasets}
    \label{fig:untuned}
\end{figure}

Figure \ref{fig:untuned} shows the off the shelf performance of BiomedCLIP over 2D datasets in CTC.


\subsubsection{Fine-Tuning Strategies}
% Fine-tuning is done on BiomedCLIP's pretrained \(BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\) model in batches of size 32, and \(1e-3\) as the learning reate, weight decay set to 0.1, for 32 epochs, using DHN-NCE loss introduced in MedCLIP-SAMv2. Model checkpoints are saved after each epoch.

% we changed these parameters in fine-tuning for the cilia dataset: 1. tuning for less/more epochs. 2. tuning with raw images vs. training with masks of cells and cilia. 3. tuning with long descriptions vs. short descriptions. 4. tuning by varying loss funciton parameters.


To investigate the impact of different fine-tuning parameters on BiomedCLIP's performance for cilia segmentation, we systematically varied several key factors. These included adjusting the number of training epochs, comparing fine-tuning on raw images versus masked images (isolating cell bodies and ciliary structures), utilizing varying lengths of textual descriptions (short versus detailed versus randomized detailed annotations with the help of ChatGPT), and experimenting with different configurations of loss function parameters. The parameter variations are summarized in Table~\ref{tab:fine-tune-params}.

\begin{table}[htbp]
    \centering
    \caption{Fine-Tuning Parameter Variations}
    \label{tab:fine-tune-params}
    \begin{tabular}{ll}
        \toprule
        \textbf{Parameter}       & \textbf{Variation Descriptions}                           \\ \midrule
        Number of Epochs         & 2 epochs vs. 32 epochs                                    \\
        Input Images             & Raw images vs. Masked images                              \\
        Textual Descriptions     & Concise vs. Detailed Vs. Randomized detailed descriptions \\
        Loss Function Parameters & Default settings vs. Adjusted weighting schemes           \\
        \bottomrule
    \end{tabular}
\end{table}
% (\texttt{BiomedCLIP-PubMedBERT\_256-vit\_base\_patch16\_224})
The fixed parameters for fine-tuning included the pretrained BiomedCLIP model, a batch size of 32, learning rate of \(1\times10^{-3}\), weight decay of 0.1, training duration of 32 epochs, and the DHN-NCE loss introduced in MedCLIP-SAMv2. Model checkpoints were saved after each epoch.



\subsubsection{DHN-NCE Loss Function}
% The Decoupled Hard Negative Noise Contrastive Estimation (DHN-NCE) loss is an enhancement of the standard InfoNCE loss used in contrastive learning. It is designed to mitigate issues such as the negative-positive-coupling (NPC) effect, which reduces learning efficiency, especially for small batch sizes. DHN-NCE achieves this by:
% \begin{itemize}

%     \item Decoupling positive samples from the denominator of the contrastive loss, improving optimization.
%     \item Incorporating hard negative sampling, which gives more importance to difficult negative samples, improving model learning.

% \end{itemize}
% The loss is computed separately for image-to-text \((Lv \rightarrow t)\) and text-to-image \((Lt \rightarrow v)\) contrastive learning and then summed:


The Decoupled Hard Negative Noise Contrastive Estimation (DHN-NCE) loss is designed to improve contrastive learning by decoupling positive samples from the denominator and introducing hard negative sampling. The loss function consists of two terms: one for image-to-text learning and another for text-to-image learning. Table \ref{tbl:param_change_effect} describes the effects of increasing or decreasing each of the loss-function parameters.


\paragraph{Loss Definition\\}

The overall DHN-NCE loss is defined as:
\begin{equation}
    L_{\text{DHN-NCE}} = L_{v \to t} + L_{t \to v}
\end{equation}

Each term is computed as follows:

\begin{equation}
    L_{v \to t} = -\sum_{i=1}^{B} \frac{I_{p,i} T_{p,i}^\top}{\tau} + \sum_{i=1}^{B} \log \left( \sum_{j \neq i} e^{I_{p,i} T_{p,j}^\top / \tau} W_{v \to t} \right)
\end{equation}

\begin{equation}
    L_{t \to v} = -\sum_{i=1}^{B} \frac{T_{p,i} I_{p,i}^\top}{\tau} + \sum_{i=1}^{B} \log \left( \sum_{j \neq i} e^{T_{p,i} I_{p,j}^\top / \tau} W_{t \to v} \right)
\end{equation}

where:
\begin{itemize}
    \item \( I_{p,i} \) and \( T_{p,i} \) are the normalized image and text features.
    \item \( B \) is the batch size.
    \item \( \tau \) is the temperature parameter controlling the sharpness of the distribution.
    \item \( W_{v \to t} \) and \( W_{t \to v} \) are hardness weights for negative samples.
\end{itemize}

\paragraph{Hardness Weighting Factors\\}

The hardness weighting factors are defined as:

\begin{equation}
    W_{v \to t} = (B-1) \times \frac{e^{\beta_1 I_{p,i} T_{p,j}^\top / \tau}}{\sum_{k \neq i} e^{\beta_1 I_{p,i} T_{p,k}^\top / \tau}}
\end{equation}

\begin{equation}
    W_{t \to v} = (B-1) \times \frac{e^{\beta_2 T_{p,i} I_{p,j}^\top / \tau}}{\sum_{k \neq i} e^{\beta_2 T_{p,i} I_{p,k}^\top / \tau}}
\end{equation}


\begin{table}
    \caption{Effects of parameter changes on DHN-NCE loss.}
    \centering
    % \renewcommand{\arraystretch}{1.2}
    \label{tbl:param_change_effect}
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Parameter}                & \textbf{Effect of Increasing}           & \textbf{Effect of Decreasing}          \\
        \hline
        Temperature (\( \tau \))          & Smoother similarity distribution        & Sharper contrast, possible instability \\
        \hline
        Hardness (\( \beta_1, \beta_2 \)) & Emphasizes difficult negatives          & Less focus on hard negatives           \\
        \hline
        Positive Weight (\( \alpha \))    & More weight on positives, less contrast & Stronger negative differentiation      \\
        \hline
    \end{tabular}
\end{table}


\subsubsection{Evaluation of BiomedCLIP Predictions}

To quantitatively assess the accuracy of the generated heatmaps, a thresholding method was required prior to computing evaluation metrics such as Dice coefficient and Intersection over Union (IoU) against ground-truth masks. Our experiments indicated that the threshold selection critically influences the evaluation outcomes, and the optimal threshold value differs markedly across various fine-tuning configurations. Specifically, models with minimal or no fine-tuning necessitated relatively lower thresholds, whereas models subjected to extensive fine-tuning required comparatively higher thresholds to achieve optimal segmentation performance.


\section{Results and Discussion}

% Figure \ref{fig:untuned_short_med_long} depicts the performance of off-the-shelf BiomedCLIP over a sample of our cilia dataset. The same prompts were used during inference for all sample images. \textit{short}, \textit{med}, and \textit{detailed} prompts correspond to \textit{"respiratory cilia"}, \textit{"cilia on nasal epithelial cells"}, and \textit{"normal or abnormal cilia in nasal epithelial biopsy. some cilia are sticking out of the border of the cell making them visible, some other are overlapping the cell making them harder to detect"}. un-tuned BiomedCLIP struggled to detect ciliary regions.




% our experiments showed that fine-tuning the model on full images severely damages the accuracy of the model making it incapable of localizing the region of interest in any of the test. we performed fine-tuning for 2 and 32 epochs and the results were the same, although the heatmaps generated after 32 epochs were lighting up more specific regions.

% training on full size vs small size didn't make any difference.
% more epochs = throwig weight off
% training with validation set causes overfitting and the model to perform terribly.
% short descriptions + short prompts were more consistent. long descriptions led to more false positives and false negatives.
% no significant impact for changing beta 1 or 2.
% increasing temperature or alpha = no significant difference except throwing the model off on some hard to detect cilia on some images.
% increasing both temp and alpha produced inconsistency and reduced reproducibility.

% --temperature-dhnnce 0.1  --alpha-dhnnce 0.9  --beta1-dhnnce 0.95  --beta2-dhnnce 0.05  more sensitive to text prompt differences. med prompts were terrible

% --temperature-dhnnce 0.1 \
% --alpha-dhnnce 0.9 \
% --beta1-dhnnce 0.95 \
% --beta2-dhnnce 0.05 and long captions while training + short prompts

% --temperature-dhnnce 0.1 \
% --alpha-dhnnce 0.9 \
% --beta1-dhnnce 0.65 \
% --beta2-dhnnce 0.65 and long captions = best performance. more ability to detect different points. less false negatives more false positives.

% --temperature-dhnnce 0.1 \
% --alpha-dhnnce 0.9 \
% --beta1-dhnnce 0.95 \
% --beta2-dhnnce 0.15 i think the best is training with long captions ptompting with short texts

% \subsubsection{varying prompts}
% short to the point prompts =less errors i.e. less false positives, more aligned with RoI
% training on short captions requires short prompts. training on long captions work better with more detailed prompts.

Figure \ref{fig:untuned_short_med_long} depicts the performance of off-the-shelf BiomedCLIP over a sample of our cilia dataset. The same prompts were used during inference for all sample images. \textit{short}, \textit{med}, and \textit{detailed} prompts correspond to \textit{"respiratory cilia"}, \textit{"cilia on nasal epithelial cells"}, and \textit{"normal or abnormal cilia in nasal epithelial biopsy. some cilia are sticking out of the border of the cell making them visible, some other are overlapping the cell making them harder to detect"}. Un-tuned BiomedCLIP struggled to detect ciliary regions consistently across different prompt lengths.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/sam/untuned short med long.png}
    \caption{Performance of off-the-shelf BiomedCLIP with short, medium, and detailed prompts, demonstrating poor localization without fine-tuning.}
    \label{fig:untuned_short_med_long}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{figures/sam/fine-tuned no mask ep 32.png}
    \caption{Localization results after fine-tuning BiomedCLIP for 32 epochs on full images without masks, showing incorrect and overly specialized localization.}
    \label{fig:fine_tuned_no_mask_ep32}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{figures/sam/fine-tuned 1ep with mask short caption th 0.4.png}
    \caption{Improved ciliary region localization after fine-tuning BiomedCLIP for one epoch using masked images with short captions.}
    \label{fig:fine_tuned_mask_short_ep1}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{figures/sam/fine-tuned masks small train short desc ep5.png}
    \caption{Effective and balanced localization achieved by fine-tuning BiomedCLIP with a reduced dataset size (small train) using masked images and short captions for 5 epochs.}
    \label{fig:fine_tuned_mask_small_short_ep5}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{figures/sam/train with long captions.png}
    \caption{Localization results demonstrating increased flexibility but higher false positives when fine-tuning BiomedCLIP using long captions.}
    \label{fig:train_long_captions}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{figures/sam/fine-tuned with mask th 0.4.png}
    \caption{Localization impact of higher temperature (\(\tau = 0.4\)) during fine-tuning, resulting in blurred similarity distributions and compromised accuracy.}
    \label{fig:fine_tuned_with_mask_th_0_4}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{figures/sam/fine-tuned masks long captions small train 01 09 095 005.png}
    \caption{Impact of increased hardness parameters (\(\beta_1=0.65\), \(\beta_2=0.65\)) during fine-tuning, showing improved localization diversity but higher false-positive rates.}
    \label{fig:fine_tuned_masks_small_train_01_09_095_005}
\end{figure}



This study systematically evaluated the performance of BiomedCLIP integrated with SAM for segmenting ciliary regions from nasal epithelial biopsy videos. The experiments were designed to explore critical factors affecting segmentation accuracy and reproducibility, including fine-tuning duration, image processing methods (raw vs. masked), and textual prompts.

\subsection{Impact of Fine-tuning BiomedCLIP}

Initial experiments revealed substantial limitations in off-the-shelf BiomedCLIP performance. Without fine-tuning, the model frequently failed to accurately localize ciliary regions, particularly when using longer and more detailed textual prompts. Figure \ref{fig:untuned_short_med_long} clearly illustrates the poor localization performance using un-tuned BiomedCLIP with various prompt lengths. Short prompts ("respiratory cilia") yielded slightly more consistent but generally inadequate results compared to medium ("cilia on nasal epithelial cells") and detailed prompts.

Our fine-tuning experiments demonstrated significant improvements when specific conditions were met. Fine-tuning BiomedCLIP using full-sized images proved detrimental, significantly impairing model localization capabilities regardless of the number of training epochs. Conversely, fine-tuning using masked images drastically improved localization accuracy. Our findings showed that training beyond a certain epoch threshold (e.g., 32 epochs) did not enhance performance; instead, it introduced model overfitting and degraded generalizability, indicated by increasingly sparse and overly specific activation maps.

\subsection{Effects of Textual Prompts}

The choice of textual prompts substantially impacted model performance. Short and concise textual prompts generally produced more consistent and fewer false-positive results, aligning more accurately with regions of interest (ROIs). Conversely, longer and overly detailed prompts introduced ambiguity, leading to a higher incidence of both false positives and false negatives. Importantly, our observations revealed that consistency between training captions and inference prompts played a pivotal role: training with brief, concise captions required correspondingly concise prompts during inference, whereas models trained on detailed captions exhibited superior performance when detailed prompts were also utilized.

\subsection{Hyperparameter Optimization and Stability}

We also explored the influence of hyperparameters related to temperature and DHNNCE loss parameters (alpha, beta1, beta2) on segmentation stability and accuracy. Results indicated minimal sensitivity to variations in beta1 and beta2 within a moderate range; however, more extreme values (e.g., beta1=0.95, beta2=0.05) significantly heightened sensitivity to textual prompt differences, leading to instability and inconsistency. Specifically, using a balanced beta configuration (beta1=0.65, beta2=0.65) combined with long captions during training delivered optimal performance by maximizing detection accuracy and minimizing false negatives.

\subsection{Prompt Selection Strategies for SAM}

Critically, our analysis underscored the importance of effective prompt selection strategies for SAM based on BiomedCLIP-generated saliency maps. The common practice of randomly selecting points from the largest contour was found inadequate for sparse, distributed ROIs. Our findings emphasized that a more nuanced approach, considering multiple salient contours, significantly enhanced segmentation accuracy, particularly for complex images with overlapping or unclear ROIs.

In summary, these experiments highlight the necessity of strategic fine-tuning and prompt selection. Optimal segmentation outcomes hinge on carefully chosen training procedures, prompt consistency, and strategic spatial prompt selection, all tailored to specific biomedical imaging conditions.

\section{Conclusion and Final Remarks}

This chapter explored the potential of foundation models, particularly the Segment Anything Model (SAM), for biomedical image segmentation tasks. It highlighted the limitations inherent in supervised and unsupervised segmentation methods, particularly their dependence on extensive labeled datasets and poor transferability, respectively. Foundation models, specifically SAM, offer promising solutions through their remarkable zero-shot segmentation capabilities enabled by minimal user-defined prompts such as points or bounding boxes.

Our investigations demonstrated that minimal fine-tuning of a vision-foundation model significantly improves the stability and accuracy of saliency maps. These refined saliency maps serve as effective foundations for generating precise points or bounding boxes that serve as inputs for SAM. The experiments clearly underscored the criticality of the prompt selection method, showing that the strategy employed by MedclipSAMv2, which selects random points from only the largest contour in a thresholded saliency map, is not universally optimal. In cases where regions of interest (ROIs) are sparse and widely distributed, selecting multiple contours rather than focusing exclusively on the largest one can yield better segmentation performance. Furthermore, our results indicated that the accuracy and robustness of the underlying vision-foundation model, such as BiomedCLIP, heavily influence the optimality of selecting single or multiple contours.

Additionally, our findings emphasize that SAM can indeed be utilized effectively off-the-shelf for biomedical segmentation tasks under specific conditions, particularly when ROIs are clearly defined, enclosed, non-overlapping, and devoid of significant imaging artifacts. Nevertheless, fine-tuning SAM remains essential for achieving consistently reliable segmentation across diverse biomedical modalities and imaging conditions.

In conclusion, a carefully fine-tuned vision-foundation model coupled with strategic selection of points or bounding boxes considerably enhances SAM's segmentation capabilities. Future studies should further investigate optimal point or bounding box selection strategies tailored to the characteristics of specific biomedical imaging modalities. Such enhancements, along with necessary fine-tuning of SAM itself, will pave the way toward more robust, reliable, and generalizable biomedical image segmentation frameworks.
\end{document}